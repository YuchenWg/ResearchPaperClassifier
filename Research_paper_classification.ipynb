{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Research paper classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K3mNRtAkQGS"
      },
      "source": [
        "This model combines the predictions from BERT and the predictions GCN for node classification on the references. The BERT model is first ran and the output probabilities(logits) are extracted, this is then combined with (added to) the logits from the GCN and the softmax function is applied on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqURjrLsDlAZ"
      },
      "source": [
        "Install huggingface for BERT and dgl for GCN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV15ljP9DTgY"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install dgl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPVn5V5LDqXE"
      },
      "source": [
        "Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su2zpAcYDsbc"
      },
      "source": [
        "# Generic libraries\n",
        "import random\n",
        "import itertools\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, \n",
        "                                                           SequentialSampler\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# GCN libraries\n",
        "from dgl import DGLGraph\n",
        "import networkx as nx\n",
        "from dgl.nn.pytorch import GraphConv\n",
        "\n",
        "#BERT libraries\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hAy06j9DxPj"
      },
      "source": [
        "Load dataset from google drive (To run it you'll have to load the dataset from your own drive)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xqxCRiBD3cd"
      },
      "source": [
        "#drive.mount('/content/drive')\n",
        "root_path = 'drive/My Drive/homework2/'\n",
        "\n",
        "train = pd.read_csv(root_path + \"train.csv\")\n",
        "text = pd.read_csv(root_path+\"text.csv\")\n",
        "reference = pd.read_csv(root_path+\"reference.csv\")\n",
        "test = pd.read_csv(root_path+\"test.csv\")\n",
        "sample = pd.read_csv(root_path+\"sample.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86UN6AunOLxo"
      },
      "source": [
        "Create pandas dataframe with train and text and extract values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuO8EECjORQ9"
      },
      "source": [
        "train_text = pd.merge(train, text, on=\"id\")\n",
        "test_text = pd.merge(test, text, on=\"id\")\n",
        "\n",
        "titles = train_text.title.values\n",
        "labels = train_text.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0jKIpKJkE7e"
      },
      "source": [
        "**BERT MODEL**\n",
        "\n",
        "The code in this section is partially adapted from the huggingface tutorial at https://huggingface.co/transformers/quickstart.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0wHUfncYwHS"
      },
      "source": [
        "**BERT** tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxYQgU_DYvin"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased',\n",
        "                                                      do_lower_case=True)\n",
        "input_ids = []\n",
        "for title in titles:\n",
        "    encoded_title = tokenizer.encode(\n",
        "    title, \n",
        "    add_special_tokens = True,\n",
        "    max_length = 48 ,)\n",
        "    input_ids.append(encoded_title)\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=48, dtype=\"long\", \n",
        " value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "attention_masks = []\n",
        "for sent in input_ids:\n",
        "     att_mask = [int(token_id > 0) for token_id in sent]\n",
        "     attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM_61tkzdjqO"
      },
      "source": [
        "Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrBByxwldjPd"
      },
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels = \n",
        "         train_test_split(input_ids, labels, random_state=2020, test_size=0.20)\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        " random_state=2020, test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdKhy8Tid4uv"
      },
      "source": [
        "Converting to pytorch datatype and create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk-PRkI3eDvR"
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, \n",
        "                                                     batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, \n",
        "                                                           validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, \n",
        "                                                        batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5hjP8q3ecq4"
      },
      "source": [
        "**BERT** model definition (Hyperparameters are tuned here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCZ9ebdJebxZ"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-large-uncased\", \n",
        "    num_labels = 5,)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        " lr = 1.5e-5,\n",
        " eps = 1e-8 )\n",
        "\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        " num_warmup_steps = 0,\n",
        " num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Fy234qjLrT"
      },
      "source": [
        "Define helper functions for BERT training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ5Bef6MjKxF"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        " pred_flat = np.argmax(preds, axis=1).flatten()\n",
        " labels_flat = labels.flatten()\n",
        " return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def flat_pred(preds):\n",
        " pred_flat = np.argmax(preds, axis=1).flatten()\n",
        " return pred_flat\n",
        "\n",
        "def format_time(elapsed):\n",
        " elapsed_rounded = int(round((elapsed)))\n",
        " return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7nAYsOciDBs"
      },
      "source": [
        "**BERT** training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "917FXjLCiCj8"
      },
      "source": [
        "device = torch.device(\"cuda\")    \n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "####################Training###############################\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')    \n",
        "    t0 = time.time()    \n",
        "    total_loss = 0   \n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):    \n",
        "        if step % 40 == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, \n",
        "                                                len(train_dataloader), elapsed))        \n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)       \n",
        "\n",
        "        model.zero_grad()              \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]      \n",
        "        total_loss += loss.item() \n",
        "        loss.backward()       \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)    \n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "###########################################Validation##########################\n",
        "\n",
        "    print(\"Running Validation...\")    \n",
        "    t0 = time.time() \n",
        "\n",
        "    model.eval()   \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0  \n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():                    \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy        \n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1    \n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKbEw9Y8Fc7L"
      },
      "source": [
        "Predict and write Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ_BIezFFeCg"
      },
      "source": [
        "# Create sentence and label lists\n",
        "titles = test_text.title.values\n",
        "input_ids = []\n",
        "\n",
        "for title in titles:\n",
        "    encoded_title = tokenizer.encode(\n",
        "                        title,         \n",
        "                        add_special_tokens = True,)\n",
        "    input_ids.append(encoded_title)\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=48, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "batch_size = 32  \n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler,\n",
        "                                                         batch_size=batch_size)\n",
        "\n",
        "predictions = []\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask = batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)  \n",
        "      \n",
        "  logits = outputs[0] \n",
        "  logits = logits.detach().cpu().numpy()\n",
        "\n",
        "  predictions.append(logits)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ATn10noj-EX"
      },
      "source": [
        "Extract predicted probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da7LnJpCyGXo"
      },
      "source": [
        "pred = []\n",
        "for i in range(len(predictions)):\n",
        "  pred_labels_i = np.array(predictions[i])\n",
        "  for j in pred_labels_i:\n",
        "      pred.append(j)\n",
        "\n",
        "BERT_pred = torch.Tensor(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU5W_0jyeaYN"
      },
      "source": [
        "**GCN for Node classification**\n",
        "\n",
        "For some unknown reason both GCN and GAT with input features being the paper titles have performed very poorly. The plain GCN with only the references performed the best so this will be the model to combine with BERT. \n",
        "\n",
        "The code in this section is partially adapted from the dgl tutorial at https://docs.dgl.ai/en/0.4.x/tutorials/basics/1_first.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-76bylYmaJk"
      },
      "source": [
        "Preprocessing and graph definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHQrohhnedwQ"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "all_titles = np.concatenate((train_text.title.values, test_text.title.values))\n",
        "\n",
        "labels_filled = []\n",
        "all_train_idx = []\n",
        "test_idx = []\n",
        "\n",
        "for i in range(len(all_titles)):\n",
        "    if i in train_text.id.values:\n",
        "        all_train_idx.append(i)\n",
        "        labels_filled.append(train_text.loc[train_text.id == i].label.values[0])\n",
        "    else:\n",
        "        test_idx.append(i)\n",
        "        labels_filled.append(0)\n",
        "\n",
        "# Graph\n",
        "reference.columns = ['src', 'tgt']\n",
        "G_nx = nx.Graph()\n",
        "G_nx.add_nodes_from(range(25561))\n",
        "for ind in reference.index:\n",
        "  G_nx.add_edge(reference['src'][ind], reference['tgt'][ind])\n",
        "\n",
        "G_dgl = DGLGraph(G_nx)\n",
        "\n",
        "embed = nn.Embedding(25561, 300)\n",
        "G_dgl.ndata['feat'] = embed.weight\n",
        "\n",
        "# Labels\n",
        "labels = torch.LongTensor(labels_filled)\n",
        "\n",
        "# Train test split\n",
        "valid_idx = all_train_idx[round(0.90*len(all_train_idx)):]\n",
        "train_idx = all_train_idx[:round(0.90*len(all_train_idx))]\n",
        "\n",
        "all_tr_idx = torch.tensor(all_train_idx)\n",
        "tr_idx = torch.tensor(train_idx)\n",
        "va_idx = torch.tensor(valid_idx)\n",
        "te_idx = torch.tensor(test_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERtRPAQlmiD7"
      },
      "source": [
        "GCN  and helper functions definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBytCpt_miUr"
      },
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_size, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, hidden_size)\n",
        "        self.conv2 = GraphConv(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = self.conv1(g, inputs)\n",
        "        h = torch.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    _, indices = torch.max(logits, dim=1)\n",
        "    correct = torch.sum(indices == labels)\n",
        "    return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "def predict(logits):\n",
        "    _, indices = torch.max(logits, dim=1)\n",
        "    \n",
        "    return indices\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHLvTFcxXhCA"
      },
      "source": [
        "GCN Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqIEvNOqXbuI"
      },
      "source": [
        "net = GCN(300, 128, 5)\n",
        "inputs = embed.weight\n",
        "\n",
        "optimizer = torch.optim.Adam(itertools.chain(net.parameters(), \n",
        "                                                  embed.parameters()), lr=0.01)\n",
        "\n",
        "dur = []\n",
        "for epoch in range(12):\n",
        "    logits = net(G_dgl, inputs)\n",
        "    logp = F.log_softmax(logits, 1)\n",
        "    loss = F.nll_loss(logp[tr_idx], labels[tr_idx])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch >= 3:\n",
        "        t0 = time.time()\n",
        "        dur.append(time.time() - t0)\n",
        "\n",
        "    train_acc = accuracy(logp[tr_idx], labels[tr_idx])\n",
        "    val_acc = accuracy(logp[va_idx], labels[va_idx])\n",
        "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | TrainAcc {:.4f} |\"\n",
        "              \" ValAcc {:.4f}\".\n",
        "              format(epoch, np.mean(dur), loss.item(), train_acc,\n",
        "            val_acc,))\n",
        "    \n",
        "graph_pred = logp[test_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImeMi_LjXm1A"
      },
      "source": [
        "**Model outputs combination**\n",
        "\n",
        "Combine logits from the two models for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5tF0JBtfngY"
      },
      "source": [
        "stack_prob = torch.stack([BERT_pred,graph_pred])\n",
        "final_prob = torch.sum(stack_prob,dim = 0)\n",
        "\n",
        "final_pred = predict(final_prob)\n",
        "\n",
        "test_text[\"label\"] = final_pred\n",
        "print(test_text.head())\n",
        "\n",
        "test = test_text.drop(\"title\",1)\n",
        "test.to_csv('test.csv',index=False)\n",
        "\n",
        "files.download('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}